{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to My Blog! </p> <p>I\u2019m Mahsa, a DevOps/Cloud Engineer passionate about technology, automation, and system design. I created this space to document my learning journey\u2014exploring new technologies, tackling challenges, and refining my approach to engineering and problem-solving.</p> <p>Over the years, I\u2019ve worked in diverse environments, gaining insights that shaped my technical mindset. This blog serves as a personal knowledge base, where I share lessons learned, projects I\u2019ve built, and ideas that inspire me.</p>"},{"location":"posts/kubernetes/kubectl-apply/","title":"Understanding `kubectl apply` in Kubernetes","text":"<p>Kubernetes is often seen as a world of Pods, but it's just as much a world of controllers. To understand how everything works in a Kubernetes environment, the key question to ask is:</p> <p>What happens when we apply a manifest via <code>kubectl</code> to a cluster?</p> <p>This is similar in spirit to the classic interview question:</p> <p>What happens when you type a URL into your browser?</p>","tags":["Kubernetes","kubectl"]},{"location":"posts/kubernetes/kubectl-apply/#starting-point-kubectl-apply","title":"Starting Point: <code>kubectl apply</code>","text":"<p>When we apply a manifest using <code>kubectl</code>, we are effectively sending an HTTPS request to the Kubernetes API server (<code>kube-apiserver</code>). Depending on whether the resource exists, the request could use <code>POST</code>, <code>PUT</code>, or <code>PATCH</code>. The <code>kubectl</code> client handles this intelligently using a 3-way strategic merge:</p> <ul> <li>It compares the previous configuration (last-applied), the current cluster state, and the new manifest.</li> <li>The last-applied configuration is stored in an annotation on the resource: <code>kubectl.kubernetes.io/last-applied-configuration</code>.</li> <li>This allows for non-destructive updates and idempotent operations.</li> </ul> <p>This makes <code>kubectl apply</code> different from <code>create</code> or <code>replace</code>, which respectively fail if a resource exists or overwrite everything.</p> <p>You could simulate part of this interaction using <code>curl</code>:</p> <pre><code>curl -k -H \"Authorization: Bearer &lt;TOKEN&gt;\" \\\n     -H \"Content-Type: application/yaml\" \\\n     --data-binary @manifest.yaml \\\n     https://&lt;K8S-API&gt;/api/v1/namespaces/&lt;namespace&gt;/pods\n</code></pre> <p>This command only works for creating resources and requires the correct API path:</p> <ul> <li>Deployments: <code>/apis/apps/v1/namespaces/default/deployments</code></li> <li>Services: <code>/api/v1/namespaces/default/services</code></li> </ul> <p>Once validated, the <code>kubectl</code> client sends an HTTPS request from your machine to the <code>kube-apiserver</code>, a component of the Kubernetes control plane.</p> <p>Note: Manifests can be written in <code>yaml</code> or <code>json</code>, both of which are understood by <code>kubectl</code>.</p> <p>Note: If you directly edit a resource using <code>kubectl edit</code> or <code>kubectl patch</code> (not <code>apply</code>), the <code>last-applied-configuration</code> will be outdated, and future <code>kubectl apply</code> operations might behave unexpectedly. That\u2019s why <code>kubectl apply</code> should be your primary method for managing resources in declarative workflows.</p>","tags":["Kubernetes","kubectl"]},{"location":"posts/kubernetes/kubectl-apply/#kubernetes-a-system-of-apis-and-a-key-value-store","title":"Kubernetes: A System of APIs and a Key-Value Store","text":"<p>The <code>kube-apiserver</code> serves as the frontend to the control plane and handles all REST API requests. It performs several steps:</p> <ul> <li>Authentication: Confirms that the requester is valid (via RBAC, service accounts, etc.).</li> <li>Authorization: Checks if the requester is allowed to perform the action.</li> <li>Validation: Verifies that the request complies with the Kubernetes schema.</li> <li>Admission Control: Applies policies (e.g., quotas, security checks).</li> </ul> <p>If all checks pass, the <code>kube-apiserver</code> persists the resource in <code>etcd</code>, the distributed key-value store that acts as Kubernetes' source of truth.</p> <p>Kubernetes operates on a declarative model, meaning you declare the desired state, and the system works to maintain it:</p> <ul> <li>For reads: the API server fetches data from <code>etcd</code>.</li> <li>For writes: the API server stores new or updated objects in <code>etcd</code>.</li> </ul>","tags":["Kubernetes","kubectl"]},{"location":"posts/kubernetes/kubectl-apply/#the-3-way-reconciliation-loop-api-server-etcd-controllers","title":"The 3-Way Reconciliation Loop: API Server, etcd, Controllers","text":"<p>At the heart of Kubernetes lies its reconciliation loop, which acts like a feedback-driven 3-way handshake between:</p> <ol> <li>You (Desired State) \u2014 you declare what you want via <code>kubectl apply</code>.</li> <li>etcd (Current State) \u2014 it stores the live state of all objects.</li> <li>Controller (Actual State Manager) \u2014 it watches the state and enforces convergence.    </li> </ol> <p>The flow looks like this:</p> <ul> <li>You apply a Deployment (desired state).</li> <li>The <code>kube-apiserver</code> stores it in <code>etcd</code>.</li> <li>The Deployment Controller, running inside the <code>kube-controller-manager</code>, watches for changes to Deployments.</li> <li>It compares desired vs actual, and if there's a difference (e.g., fewer replicas than desired), it takes action (e.g., create new ReplicaSets or Pods).    </li> <li>These actions again go through the API server and are saved into <code>etcd</code>.</li> </ul> <p>This loop continues until the actual state matches the desired state. This is the magic behind Kubernetes self-healing.</p>","tags":["Kubernetes","kubectl"]},{"location":"posts/kubernetes/kubectl-apply/#who-watches-over-kubernetes-objects","title":"Who Watches Over Kubernetes Objects?","text":"<p>Every object in Kubernetes is watched by a controller that ensures it matches the desired state:</p> <ul> <li>Pods are managed by a ReplicaSet.</li> <li>ReplicaSets are managed by a Deployment.</li> <li>These are managed by controllers running in the kube-controller-manager binary.</li> </ul> <p>So, when a Deployment is applied, the Deployment Controller observes the Deployment resource and ensures the defined number of Pods are running. If there's a mismatch between the current and desired state, the controller adjusts accordingly (e.g., scaling Pods, initiating rolling updates).</p> <p>Note: A Kubernetes Operator is a custom controller that extends Kubernetes to manage complex applications by packaging domain-specific logic.</p>","tags":["Kubernetes","kubectl"]},{"location":"posts/kubernetes/kubectl-apply/#where-does-a-pod-live-the-scheduler-decides","title":"Where Does a Pod Live? The Scheduler Decides","text":"<p>Once the Deployment Controller determines that new Pods are needed, it delegates the scheduling decision to the Scheduler. The Scheduler evaluates available Nodes using:</p> <ul> <li>Resource availability (CPU, memory)</li> <li>Taints and tolerations</li> <li>Affinity/anti-affinity rules</li> <li>Node selectors and constraints</li> </ul> <p>It then sets the <code>nodeName</code> field in the Pod spec to assign it to a specific Node.</p> <p>Note: The Scheduler only makes placement decisions; it doesn\u2019t run Pods.</p>","tags":["Kubernetes","kubectl"]},{"location":"posts/kubernetes/kubectl-apply/#when-the-pod-is-born","title":"When the Pod Is Born","text":"<p>Once scheduled, the Pod becomes part of the desired state for the Kubelet, which runs on each Node. The Kubelet:</p> <ul> <li>Pulls required container images</li> <li>Creates and starts containers</li> <li>Monitors health using liveness and readiness probes</li> <li>Sends status updates to the API server</li> </ul> <p>Health Checks: If a container fails a liveness probe, it is restarted. If it fails a readiness probe, it's temporarily removed from service.</p>","tags":["Kubernetes","kubectl"]},{"location":"posts/kubernetes/kubectl-apply/#how-does-the-kubelet-pull-images","title":"How Does the Kubelet Pull Images?","text":"<p>The Kubelet interacts with the container runtime using the Container Runtime Interface (CRI), a gRPC-based API. It connects to the runtime via a Unix socket:</p> <pre><code>/run/containerd/containerd.sock       # for containerd\n/var/run/crio/crio.sock              # for CRI-O\n</code></pre> <p>You can configure this using the <code>--container-runtime-endpoint</code> flag:</p> <pre><code>kubelet --container-runtime-endpoint=unix:///run/containerd/containerd.sock\n</code></pre> <p>To verify the connection:</p> <pre><code>sudo crictl info\n</code></pre> <p>To check which socket the Kubelet is using:</p> <pre><code>ps aux | grep kubelet\n</code></pre> <p>Flow Summary:</p> <ol> <li>Kubelet asks CRI to create a Pod sandbox (network namespace).</li> <li>CRI uses <code>runc</code> (or similar) to create it.</li> <li>Kubelet requests image download via CRI.</li> <li>Kubelet instructs CRI to start the container.</li> <li>Kubelet monitors the container via CRI.</li> </ol>","tags":["Kubernetes","kubectl"]},{"location":"posts/kubernetes/kubectl-apply/#pod-lifecycle-and-maintenance","title":"Pod Lifecycle and Maintenance","text":"<p>Once running, a Pod goes through different phases:</p> Phase Description <code>Pending</code> Accepted by the cluster, waiting to be scheduled or for images to download. <code>Running</code> Scheduled to a node, with at least one running container. <code>Succeeded</code> All containers exited successfully and won\u2019t restart. <code>Failed</code> All containers exited, and at least one failed. <code>Unknown</code> Pod state cannot be determined, often due to communication issues. <p>Kubernetes handles restarts via <code>restartPolicy</code>:</p> <ul> <li>Initial crash: Container restarts immediately.</li> <li>Repeated crashes: Backoff delays increase exponentially.</li> <li>CrashLoopBackOff: Indicates repeated failures with backoff in place.</li> <li>Backoff reset: If a container runs successfully for a set time, backoff resets.</li> </ul> <p>Evicted Pods: Sometimes, Pods are terminated and marked as <code>Evicted</code> due to node-level resource pressure (e.g., memory or disk). This is not a failure in the Pod itself, but a protective measure by the kubelet. These Pods are removed to reclaim resources, and depending on the controller (e.g., Deployment or StatefulSet), they are typically recreated on another node.</p>","tags":["Kubernetes","kubectl"]},{"location":"posts/kubernetes/kubectl-apply/#what-about-kube-proxy","title":"What about kube-proxy?","text":"<p>We talked about other components of Kubernetes but what about kube-proxy? <code>kube-proxy</code> does not directly involved during the <code>kubectl apply</code>But it plays a critical role afterward, once your Deployment results in Pods and Services being created.</p> <p>kube-proxy is the component that manages networking rules on each node to route traffic to the appropriate backend Pods. It handles:</p> <ul> <li>Virtual IPs (ClusterIPs) for Services</li> <li>NAT rules using <code>iptables</code>, <code>ipvs</code>, or <code>ebpf</code> (depending on config)</li> <li>Load balancing between Pods behind a Service</li> <li>NodePort exposure for Services if configured</li> </ul> <p>After the Pods from your Deployment are up and running and a Service is created, kube-proxy:</p> <ul> <li>Detects the creation of the Service (via watching the API Server).</li> <li>Sets up routing rules (e.g., iptables) so that:</li> <li>Requests to the Service IP and port are load balanced to backend Pods.</li> <li>Continuously watches for changes in Pod IPs and updates routing accordingly.</li> </ul> <p>Note: For Pod-level networking (IP assignment, Pod-to-Pod reachability), Kubernetes uses CNI plugins (e.g., Calico, Flannel, Cilium). kube-proxy sits on top of that for Service abstraction.</p>","tags":["Kubernetes","kubectl"]},{"location":"posts/kubernetes/kubectl-apply/#events-and-observability","title":"Events and Observability","text":"<p>Kubernetes is event-driven. During every step \u2014 from applying manifests to scheduling, pulling images, or crashing \u2014 Kubernetes components emit Events. These are stored in the API server and visible via:</p> <pre><code>kubectl describe pod &lt;pod-name&gt;\n</code></pre> <p>You'll often see logs like:</p> <ul> <li>\"Successfully assigned pod...\"</li> <li>\"Started container...\"</li> <li>\"Back-off restarting failed container...\"</li> </ul> <p>These are critical for debugging the lifecycle of resources.</p>","tags":["Kubernetes","kubectl"]},{"location":"posts/kubernetes/kubectl-apply/#to-summarize","title":"To summarize","text":"<p>This entire process demonstrates how Kubernetes self-heals, maintains state, and abstracts away the operational complexity of deploying and running containers at scale.</p> <p>Think of it like this:</p> <ul> <li>You (<code>kubectl</code>) send a construction plan to the city (<code>API server</code>).</li> <li>Builders (<code>controllers</code>, <code>scheduler</code>, <code>kubelet</code>) build the house (<code>Pods</code>, <code>ReplicaSets</code>).</li> <li>kube-proxy is the traffic engineer: it comes in after the houses are built and makes sure people (network traffic) know how to get there.</li> </ul> <p>And all of it is made possible by the feedback loop between your declaration, the control plane, and the controllers that keep things aligned \u2014 always striving for desired state = actual state.</p>","tags":["Kubernetes","kubectl"]},{"location":"posts/linux/linux-netns/","title":"Linux Network NameSpaces","text":"<p>Imagine you have a container (like a Docker container) running a process. When you run <code>ps aux</code> inside the container, you'll see the process running. But if you run the same command on the host, you\u2019ll see the same process with a different <code>PID</code>. This happens because of Linux namespaces, a powerful kernel feature that isolates system resources.</p>"},{"location":"posts/linux/linux-netns/#what-are-namespaces","title":"What Are Namespaces?","text":"<p>Namespaces partition kernel resources so that different sets of processes see different views of the system. Essentially, a namespace provides an isolated environment where certain system resources\u2014like process IDs (PIDs), hostnames, user IDs, file names, and network interfaces\u2014appear as if they belong only to the processes within that namespace. This allows multiple processes (or groups of processes) to operate independently without interfering with each other.</p> <p>The most popular use case for namespaces today is containers. Containers leverage both namespaces and cgroups:</p> <ul> <li>Namespaces provide isolation, ensuring each container has its own view of the system.</li> <li>Control Groups (cgroups) manage resource allocation, limiting CPU, memory, and I/O usage.</li> </ul> <p>Together, these features allow containers to function as lightweight, isolated environments, benefiting both development and operations teams.</p>"},{"location":"posts/linux/linux-netns/#namespace-isolation-in-action","title":"Namespace Isolation in Action","text":"<p>Let's go back to our container example. The host OS can see all running processes, including those inside containers, but the reverse is not true\u2014the container has no visibility into the host's processes. This is because the containerized process is running inside a PID namespace, which gives it a different process ID than what appears on the host.</p> <p>Now, let's consider network isolation. The host machine has its own network interface (typically named <code>eth0</code>), which connects to a LAN and manages network resources such as iptables and routing tables. When a new container is created, it gets its own network namespace, meaning it operates as if it has a separate network stack.</p> <p>Inside the container, instead of using <code>eth0</code>, the container is assigned a virtual Ethernet interface\u2014let's call it <code>veth0</code>. This interface is paired with another virtual interface on the host, allowing network communication between the two.</p>"},{"location":"posts/linux/linux-netns/#understanding-veth-pairs-as-a-network-pipe","title":"Understanding veth Pairs as a Network Pipe","text":"<p>A veth (virtual Ethernet) pair consists of two linked virtual interfaces that act as a tunnel between two different network namespaces. When you send packets into one <code>veth</code> interface, they immediately appear at the other end of the pair.</p>"},{"location":"posts/linux/linux-netns/#how-it-works","title":"How It Works","text":"<ol> <li> <p>Create Two Network Namespaces (<code>red</code> and <code>blue</code>):</p> <pre><code>ip netns add red\nip netns add blue\n</code></pre> </li> <li> <p>Create a Virtual Ethernet (veth) Pair (<code>veth-red</code> and <code>veth-blue</code>):</p> <pre><code>ip link add veth-red type veth peer name veth-blue\n</code></pre> <ul> <li>This creates two linked virtual interfaces: <code>veth-red</code> and <code>veth-blue</code>.</li> </ul> </li> <li> <p>Move Each Interface to Its Namespace:</p> <pre><code>ip link set veth-red netns red\nip link set veth-blue netns blue\n</code></pre> <ul> <li>Now <code>veth-red</code> exists inside <code>red</code> and <code>veth-blue</code> inside <code>blue</code>.</li> </ul> </li> <li> <p>Assign IP Addresses:</p> <pre><code>ip netns exec red ip addr add 192.168.1.1/24 dev veth-red\nip netns exec blue ip addr add 192.168.1.2/24 dev veth-blue\n</code></pre> <ul> <li>Each end of the <code>veth</code> pair gets an IP address.</li> </ul> </li> <li> <p>Bring the Interfaces Up:</p> <pre><code>ip netns exec red ip link set veth-red up\nip netns exec blue ip link set veth-blue up\n</code></pre> <ul> <li>This makes the interfaces active.</li> </ul> </li> <li> <p>Enable Loopback in Each Namespace:</p> <pre><code>ip netns exec red ip link set lo up\nip netns exec blue ip link set lo up\n</code></pre> </li> <li> <p>Test Connectivity:</p> <pre><code>ip netns exec red ping -c 3 192.168.1.2\n</code></pre> <ul> <li>This should successfully ping <code>blue</code>.</li> </ul> </li> </ol>"},{"location":"posts/linux/linux-netns/#why-this-works-as-a-pipe","title":"Why This Works as a \"Pipe\"?","text":"<ul> <li>Point-to-Point Connection: The <code>veth</code> pair acts as a direct link, just like a network cable.</li> <li>Packet Forwarding: Packets entering <code>veth-red</code> appear at <code>veth-blue</code> and vice versa.</li> <li>Independent Network Stacks: Each namespace sees only its assigned virtual interface, isolating network environments.</li> </ul>"},{"location":"posts/linux/linux-netns/#connecting-multiple-network-namespaces","title":"Connecting Multiple Network Namespaces","text":"<p>Managing multiple network namespaces on a single host can become inefficient when creating a direct veth pair for each connection. A more scalable solution involves using a virtual switch to interconnect the namespaces, which can be accomplished with tools like Open vSwitch or Linux Bridge.</p> <p>To efficiently connect more than two namespaces, a Linux bridge (<code>v-net-0</code>) can serve as a central switch-like component, enabling seamless communication between namespaces. Each namespace will have a veth pair: one end inside the namespace and the other attached to the bridge, allowing the namespaces to communicate as if they were on the same network.</p>"},{"location":"posts/linux/linux-netns/#steps-to-set-up-multiple-network-namespaces","title":"Steps to Set Up Multiple Network Namespaces:","text":"<ol> <li> <p>Create a Linux Bridge:</p> <pre><code>ip link add name v-net-0 type bridge\nip link set v-net-0 up\n</code></pre> </li> <li> <p>Create and Attach veth Pairs for Each Namespace:</p> <pre><code>for ns in red blue green; do\n    ip netns add $ns\n    ip link add veth-$ns type veth peer name $ns-veth\n    ip link set $ns-veth netns $ns\n    ip link set veth-$ns master v-net-0\n    ip link set veth-$ns up\ndone\n</code></pre> </li> <li> <p>Assign IP Addresses and Bring Interfaces Up:</p> <pre><code>ip netns exec red ip addr add 192.168.1.1/24 dev red-veth\nip netns exec blue ip addr add 192.168.1.2/24 dev blue-veth\nip netns exec green ip addr add 192.168.1.3/24 dev green-veth\n\nfor ns in red blue green; do\n    ip netns exec $ns ip link set $ns-veth up\n    ip netns exec $ns ip link set lo up\ndone\n</code></pre> </li> </ol> <p>At this point, all namespaces (<code>red</code>, <code>blue</code>, <code>green</code>) are connected through the <code>v-net-0</code> bridge and can communicate freely.</p>"},{"location":"posts/linux/linux-netns/#accessing-namespaces-from-the-host","title":"Accessing Namespaces from the Host","text":"<p>Now, the namespaces are part of a private network where they can ping each other. However, to ping a namespace from the host, you need to assign an IP address to the bridge (<code>v-net-0</code>).</p> <p>To allow the host to communicate with the namespaces, assign an IP to <code>v-net-0</code>:</p> <pre><code>ip addr add 192.168.1.254/24 dev v-net-0\n</code></pre> <p>With this setup, the network is isolated within the host, meaning external devices cannot access the namespaces directly. The only entry point is <code>eth0</code> on the host.</p> <p>If you need to enable external communication, add routing rules to ensure that namespaces recognize the host as their gateway. Since the host acts as a gateway between the private network and external networks, you can add the host\u2019s IP as a route:</p> <pre><code>ip netns exec blue ip route add 192.168.1.0/24 via 192.168.1.254\n</code></pre>"},{"location":"posts/linux/linux-netns/#nat-configuration-for-external-connectivity","title":"NAT Configuration for External Connectivity","text":"<p>If there are multiple interfaces on the host (such as <code>eth0</code> and <code>v-net-0</code>), a routing issue can arise. Requests from the <code>blue</code> namespace may fail to return to the host due to the way packets are routed. To address this, configure NAT (Network Address Translation) on the host:</p> <pre><code>iptables -t nat -A POSTROUTING -s 192.168.1.0/24 -o eth0 -j MASQUERADE\n</code></pre> <p>To enable namespaces to access external networks, define a default gateway:</p> <pre><code>ip netns exec red ip route add default via 192.168.1.254\nip netns exec blue ip route add default via 192.168.1.254\n</code></pre>"},{"location":"posts/linux/linux-netns/#port-forwarding-for-namespace-services","title":"Port Forwarding for Namespace Services","text":"<p>To allow external access to services running within a namespace, set up port forwarding using <code>iptables</code>. For example, to forward traffic from port 8080 in the <code>red</code> namespace to port 80 on the host:</p> <pre><code>iptables -t nat -A PREROUTING -p tcp --dport 8080 -j DNAT --to-destination 192.168.1.1:80\niptables -A FORWARD -p tcp -d 192.168.1.1 --dport 80 -j ACCEPT\n</code></pre> <p>This configuration allows external users to access services running inside the namespace by reaching the host's port 8080.</p> <p>This version improves readability and structure while retaining all critical information.</p>"},{"location":"posts/linux/linux-netns/#conclusion","title":"Conclusion","text":"<p>Linux namespaces, combined with <code>veth</code> pairs and bridges, enable powerful network isolation and connectivity. These concepts form the backbone of containerized networking, allowing for isolated yet flexible communication structures across different applications and environments.</p>"}]}